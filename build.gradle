plugins {
  id 'java'
  id 'net.saliman.properties' version '1.5.2'
  id 'com.github.johnrengelman.shadow' version '8.1.1'
  id "com.github.jk1.dependency-license-report" version "1.19"

  // Only used for testing
  id 'com.marklogic.ml-gradle' version '4.7.0'
  id 'jacoco'
  id "org.sonarqube" version "4.4.1.3373"

  // Used to generate Avro classes. This will write classes to build/generated-test-avro-java and also add that folder
  // as a source root. Since this is commented out by default, the generated Avro test class has been added to
  // src/test/java. This only needs to be uncommented when there's a need to regenerate that class, at which point it
  // should be copied over to src/test/java and then this plugin should be commented out again.
  // id "com.github.davidmc24.gradle.plugin.avro" version "1.6.0"
}

java {
  sourceCompatibility = 1.8
  targetCompatibility = 1.8
}

repositories {
  mavenCentral()
}

configurations {
  documentation
  assets
}

ext {
  // Even though Kafka Connect 3.7.0 is out, we're staying with 3.6.1 in order to continue
  // using the third-party Kafka JUnit tool. See https://github.com/mguenther/kafka-junit?tab=readme-ov-file
  kafkaVersion = "3.6.1"
}

dependencies {
  compileOnly "org.apache.kafka:connect-api:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-json:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-runtime:${kafkaVersion}"
  compileOnly "org.slf4j:slf4j-api:2.0.13"

  implementation 'com.marklogic:ml-javaclient-util:4.7.0'
  // Force DHF to use the latest version of ml-app-deployer, which minimizes security vulnerabilities
  implementation "com.marklogic:ml-app-deployer:4.7.0"

  implementation "com.fasterxml.jackson.dataformat:jackson-dataformat-csv:2.15.3"

  // Note that in general, the version of the DHF jar must match that of the deployed DHF instance. Different versions
  // may work together, but that behavior is not guaranteed.
  implementation("com.marklogic:marklogic-data-hub:5.8.0") {
    exclude module: "marklogic-client-api"
    exclude module: "ml-javaclient-util"
    exclude module: "ml-app-deployer"

    // No need for mlcp-util, it's only used in 'legacy' DHF 4 jobs
    exclude module: "mlcp-util"
    // Excluding because it causes Kafka Connect to complain mightily if included
    exclude module: "logback-classic"
  }

  testImplementation 'com.marklogic:marklogic-junit5:1.4.0'

  testImplementation "org.apache.kafka:connect-api:${kafkaVersion}"
  testImplementation "org.apache.kafka:connect-json:${kafkaVersion}"
  testImplementation 'net.mguenther.kafka:kafka-junit:3.6.0'

  testImplementation "org.apache.avro:avro-compiler:1.11.3"

  // Forcing logback to be used for test logging
  testImplementation "ch.qos.logback:logback-classic:1.3.14"
  testImplementation "org.slf4j:jcl-over-slf4j:2.0.13"

  documentation files('LICENSE.txt')
  documentation files('NOTICE.txt')
  documentation files('README.md')

  assets files('MarkLogic_logo.png')
  assets files('apache_logo.png')
}

// This ensures that the compiler reports "unchecked" warnings.
// This helps us use the compiler to prevent potential problems.
tasks.withType(JavaCompile) {
  options.compilerArgs << '-Xlint:unchecked'
  options.deprecation = true
}

test {
  useJUnitPlatform()
}

// Configures jacoco test coverage to be included when "test" is run
test {
  finalizedBy jacocoTestReport
}
jacocoTestReport {
  dependsOn test
}
// Enabling the XML report allows for sonar to grab coverage data from jacoco
jacocoTestReport {
  reports {
    // This isn't working with Gradle 8. Will replace this soon with the sonar instance in docker-compose.
    // xml.enabled true
  }
}


shadowJar {
  // Exclude DHF source files
  exclude "hub-internal-artifacts/**"
  exclude "hub-internal-config/**"
  exclude "ml-config/**"
  exclude "ml-modules*/**"
  exclude "scaffolding/**"
}

task copyJarToKafka(type: Copy, dependsOn: shadowJar) {
  description = "Used for local development and testing; copies the jar to your local Kafka install"
  from "build/libs"
  into "${kafkaHome}/libs"
}

task copyPropertyFilesToKafka(type: Copy) {
  description = "Used for local development and testing; copies the properties files to your local Kafka install"
  from "config"
  into "${kafkaHome}/config"
  filter { String line ->
    line.startsWith('ml.connection.username=') ? 'ml.connection.username=' + kafkaMlUsername : line
  }
  filter { String line ->
    line.startsWith('ml.connection.password=') ? 'ml.connection.password=' + kafkaMlPassword : line
  }
}

task deploy {
  description = "Used for local development and testing; builds the jar and copies it and the properties files to your local Kafka install"
  dependsOn = ["copyJarToKafka", "copyPropertyFilesToKafka"]
}

ext {
  confluentArchiveGroup = "Confluent Connector Archive"
  confluentTestingGroup = "Confluent Platform Local Testing"
  baseArchiveBuildDir = "build/connectorArchive"
  baseArchiveName = "${componentOwner}-${componentName}-${version}"
}

// Tasks for building the archive required for submitting to the Confluence Connector Hub

import org.apache.tools.ant.filters.ReplaceTokens

task connectorArchive_CopyManifestToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project manifest into the root folder"
  from '.'
  include 'manifest.json'
  into "${baseArchiveBuildDir}/${baseArchiveName}"
  filter(ReplaceTokens, tokens: [CONFLUENT_USER: componentOwner, VERSION: version])
}

task connectorArchive_CopyAssetsToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project assets into the assets folder"
  from configurations.assets
  into "${baseArchiveBuildDir}/${baseArchiveName}/assets"
}

task connectorArchive_CopyEtcToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project support files into the etc folder"
  from 'config'
  include '*'
  into "${baseArchiveBuildDir}/${baseArchiveName}/etc"
}

task connectorArchive_CopyDocumentationToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project documentation into the doc folder"
  from configurations.documentation
  into "${baseArchiveBuildDir}/${baseArchiveName}/doc"
}

task connectorArchive_CopyDependenciesToBuildDirectory(type: Copy, group: confluentArchiveGroup, dependsOn: jar) {
  description = "Copy the dependency jars into the lib folder"
  from jar
  // Confluent already includes the Jackson dependencies that this connector depends on. If the connector includes any
  // itself, and the DHF integration is used with the sink connector, then the following error will occur when DHF
  // tries to connect to the Manage API of MarkLogic:
  // java.lang.ClassCastException: com.fasterxml.jackson.datatype.jdk8.Jdk8Module cannot be cast to com.fasterxml.jackson.databind.Module
  //	at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.registerWellKnownModulesIfAvailable(Jackson2ObjectMapperBuilder.java:849)
  // stackoverflow indicates this may be due to multiple copies of Jackson being on the classpath, as Jdk8Module
  // otherwise should be castable to Module.
  // Testing has verified that excluding all "jackson-" jars still results in the connector working properly with
  // Confluent 7.3.1. This has no impact on using the connector with plain Apache Kafka which does not rely on
  // constructing this connector archive.
  from configurations.runtimeClasspath.findAll { it.name.endsWith('jar') && !it.name.startsWith("jackson-")}
  into "${baseArchiveBuildDir}/${baseArchiveName}/lib"
}

task connectorArchive_BuildDirectory(group: confluentArchiveGroup) {
  description = "Build the directory that will be used to create the Kafka Connector Archive"
  dependsOn = [
    connectorArchive_CopyManifestToBuildDirectory,
    connectorArchive_CopyDependenciesToBuildDirectory,
    connectorArchive_CopyDocumentationToBuildDirectory,
    connectorArchive_CopyEtcToBuildDirectory,
    connectorArchive_CopyAssetsToBuildDirectory
  ]
}

task connectorArchive(type: Zip, dependsOn: connectorArchive_BuildDirectory, group: confluentArchiveGroup) {
  description = 'Build a Connector Hub for the Confluent Connector Hub'
  from "${baseArchiveBuildDir}"
  include '**/*'
  archiveFileName = "${baseArchiveName}.zip"
  destinationDirectory = file('build/distro')
}

// Tasks for working with Confluent Platform running locally.
// See "Testing with Confluent Platform" in CONTRIBUTING.md

task installConnectorInConfluent(type: Copy, dependsOn: connectorArchive, group: confluentTestingGroup) {
  description = "Copies the connector's archive directory to the Docker volume shared with the Connect server"
  from "build/connectorArchive"
  into "src/test/confluent-platform-example/docker/confluent-marklogic-components"
}

task loadDatagenPurchasesConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the Datagen connector into Confluent Platform for sending JSON documents to " +
    "the 'purchases' topic"
  commandLine "curl", "-s", "-X", "POST", "-H", "Content-Type: application/json",
    "--data", "@src/test/resources/confluent/datagen-purchases-source.json", "http://localhost:8083/connectors"
}

task loadMarkLogicPurchasesSinkConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for writing data to " +
    "MarkLogic from the 'purchases' topic"
  commandLine "curl", "-s", "-X", "POST", "-H", "Content-Type: application/json",
    "--data", "@src/test/resources/confluent/marklogic-purchases-sink.json", "http://localhost:8083/connectors"
}

task loadMarkLogicPurchasesSourceConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for reading rows from " +
    "the demo/purchases view"
  commandLine "curl", "-s", "-X", "POST", "-H", "Content-Type: application/json",
    "--data", "@src/test/resources/confluent/marklogic-purchases-source.json", "http://localhost:8083/connectors"
}

task loadMarkLogicAuthorsSourceConnector(type: Exec, group: confluentTestingGroup) {
  description = "Loads a source connector that retrieves authors from the citations.xml file, which is also used for " +
    "all the automated tests"
  commandLine "curl", "-s", "-X", "POST", "-H", "Content-Type: application/json",
    "--data", "@src/test/resources/confluent/marklogic-authors-source.json", "http://localhost:8083/connectors"
}

task loadMarkLogicEmployeesSourceConnector(type: Exec, group: confluentTestingGroup) {
  commandLine "curl", "-s", "-X", "POST", "-H", "Content-Type: application/json",
    "--data", "@src/test/resources/confluent/marklogic-employees-source.json", "http://localhost:8083/connectors"
}

task insertAuthors(type: Test) {
  useJUnitPlatform()
  systemProperty "AUTHOR_IDS", authorIds
  description = "Insert a new author into the kafka-test-content database via a new citations XML document; " +
    "use e.g. -PauthorIds=7,8,9 to insert 3 new authors with IDs of 7, 8, and 9"
  include "com/marklogic/kafka/connect/source/debug/InsertAuthorsTest.class"
}

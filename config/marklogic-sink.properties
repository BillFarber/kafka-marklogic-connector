# Properties required for every Kafka connector

# The name of the connector
name=marklogic-sink

# The fully-qualified name of the connector class
connector.class=com.marklogic.kafka.connect.sink.MarkLogicSinkConnector

# The maximum number of concurrent tasks
tasks.max=1

# Comma-separated list of topics to subscribe to
topics=marklogic


# Properties defined by the MarkLogic Kafka connector

# Required; a MarkLogic host to connect to. By default, the connector uses the Data Movement SDK, and thus it will
# connect to each of the hosts in a cluster.
ml.connection.host=localhost

# Required; the port of a REST API server to connect to
ml.connection.port=8000

# Required; the authentication scheme used by the server defined by ml.connection.port; either 'DIGEST', 'BASIC', 'CERTIFICATE', 'KERBEROS', or 'NONE'
ml.connection.securityContextType=DIGEST

# MarkLogic username for 'DIGEST' and 'BASIC' authentication
ml.connection.username=

# MarkLogic password for 'DIGEST' and 'BASIC' authentication
ml.connection.password=

# Path to PKCS12 file for 'CERTIFICATE' authentication
# ml.connection.certFile=
# Password for PKCS12 file for 'CERTIFICATE' authentication
# ml.connection.certPassword=

# External name for 'KERBEROS' authentication
# ml.connection.externalName=

# Name of a database to connect to. If your REST API server has a content database matching that of the one that you
# want to write documents to, you do not need to set this.
# ml.connection.database=

# Set to 'GATEWAY' when the host identified by ml.connection.host is a load balancer.
# See https://docs.marklogic.com/guide/java/data-movement#id_26583 for more information.
# ml.connection.type=

# Set to 'true' for a simple SSL strategy that uses the JVM's default SslContext and X509TrustManager and an 'any' host verification strategy
# ml.connection.simpleSsl=true
# You must also ensure that the server cert or the signing CA cert is imported in the JVMs cacerts file.
# These commands may be used to get the server cert and to import it into your cacerts file.
# Don't forget to customize the commands for your particular case.
#   openssl x509 -in <(openssl s_client -connect <server>:8004 -prexit 2>/dev/null) -out ~/example.crt
#   sudo keytool -importcert -file ~/example.crt -alias <server> -keystore /path/to/java/lib/security/cacerts -storepass <storepass-password>

# Set to 'true' to customize how an SSL connection is created. Only supported if securityContextType is 'BASIC' or 'DIGEST'.
# ml.connection.enableCustomSsl=true
# The TLS version to use for custom SSL
# ml.connection.customSsl.tlsVersion=TLSv1.2
# The host verification strategy for custom SSL; either 'ANY', 'COMMON', or 'STRICT'
# ml.connection.customSsl.hostNameVerifier=ANY
# Set this to true for 2-way SSL; defaults to 1-way SSL
# ml.connection.customSsl.mutualAuth=true

# Specify the format of each document; either 'JSON', 'XML', 'BINARY', 'TEXT', or 'UNKNOWN'.
# If not set, MarkLogic will determine the document type based on the ml.document.uriSuffix property.
ml.document.format=JSON

# Comma-separated list of collections that each document should be written to
ml.document.collections=kafka-data

# Comma-separated list of roles and capabilities that define the permissions for each document
ml.document.permissions=rest-reader,read,rest-writer,update

# Prefix to prepend to each URI; the URI itself is a UUID
ml.document.uriPrefix=/kafka-data/

# Suffix to append to each URI; will determine the document type if ml.document.format is not set
ml.document.uriSuffix=.json

# Name of a temporal collection for documents to be inserted into
# ml.document.temporalCollection=

# Set this to true so that the name of the topic that the connector reads from is added as a collection to each
# document inserted by the connector
# ml.document.addTopicToCollections=true

# Specify a mime type for each document; typically ml.document.format will be used instead of this
# ml.document.mimeType=

# Set the strategy for generating a unique URI for each document written to MarkLogic. Defaults to 'UUID'.
# Other choices are: 'JSONPATH', 'HASH', 'KAFKA_META_HASHED', and 'KAFKA_META_WITH_SLASH'.
# ml.id.strategy=

# For use with JSONPATH and HASH; comma-separated list of paths for extracting values for the ID
# ml.id.strategy.paths=

# Sets the number of documents to be written in a batch to MarkLogic. This may not have any impact depending on how the
# connector receives data from Kafka. The connector calls flushAsync on the DMSDK WriteBatcher after processing every
# collection of records. Thus, if the connector never receives at one time more than the value of this property, then
# the value of this property will have no impact.
ml.dmsdk.batchSize=100

# Sets the number of threads used for parallel writes to MarkLogic. Similar to the batch size property above, this
# may never come into play depending on how many records the connector receives at once.
ml.dmsdk.threadCount=8

# Name of a REST transform to use when writing documents
# ml.dmsdk.transform=

# Delimited set of transform parameter names and values; example = param1,value1,param2,value2
# ml.dmsdk.transformParams=

# Delimiter for transform parameter names and values
# ml.dmsdk.transformParamsDelimiter=,

# Set to true to log at the info level the key of each record
# ml.log.record.key=true

# Set to true to log at the info level the headers of each record
# ml.log.record.headers=true

# Name of a Data Hub Framework flow to run after writing documents
# ml.datahub.flow.name=
# Comma-delimited list of step numbers in a flow to run
# ml.datahub.flow.steps=
# Set to true to log at the info level the response data from running a flow
# ml.datahub.flow.logResponse=true
